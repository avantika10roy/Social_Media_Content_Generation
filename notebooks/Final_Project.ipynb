{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6c65994-c210-47bd-944c-df8d2c3a8a76",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bb90b5-48e0-495f-a38b-20469f07089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers diffusers fastapi uvicorn torch torchvision Pillow accelerate datasets lora diffusers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22473bae-6a8a-4cc1-b080-ee43cdcf9f54",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca321c6b-2666-4442-a7dd-e7ec65d860e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from transformers import Trainer\n",
    "from datasets import load_dataset\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from diffusers import DDPMScheduler\n",
    "from torchvision.models import vgg19\n",
    "from transformers import BlipProcessor\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Resize\n",
    "from transformers import TrainingArguments\n",
    "from torchvision.transforms import Normalize\n",
    "from diffusers.utils import enable_full_lora\n",
    "from transformers import AutoModelForCausalLM\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from torchvision.transforms import functional as TF\n",
    "from transformers import BlipForConditionalGeneration\n",
    "\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f52542-561f-46d6-ba84-4173ac650e74",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9b9488-c526-40ef-b973-3cefbaedb7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model           = \"decapoda-research/llama-7b-hf\"\n",
    "image_model         = \"stabilityai/stable-diffusion-xl\"\n",
    "blip_processor_path = \"Salesforce/blip-image-captioning-base\"\n",
    "blip_model_path     = \"Salesforce/blip-image-captioning-base\"\n",
    "device              = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd42989-ff28-470a-87a1-fbb27dd5df82",
   "metadata": {},
   "source": [
    "## Defining Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc506a-9639-4343-a453-d01e2ca540c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_images = '../data/linkedin_data/linkedin_images/'\n",
    "linkedin_texts  = '../data/linkedin_data/post_data.json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a658f34a-95f8-42ed-b7ad-3b70af6d8fe9",
   "metadata": {},
   "source": [
    "## Loading Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1eada4-7ab8-44d1-b68b-58fb7aa102b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files = \"processed_dataset.json\")[\"train\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c40d2-a770-4fe2-a156-34e0473daf07",
   "metadata": {},
   "source": [
    "## Load BLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6322e622-71c6-4ed3-b150-62a772cee58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "blip_processor = BlipProcessor.from_pretrained(blip_processor_path)\n",
    "blip_model     = BlipForConditionalGeneration.from_pretrained(blip_model_path)\n",
    "\n",
    "# Move BLIP model to MPS\n",
    "blip_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1a267f-372c-485d-92a9-eb20f8d39f89",
   "metadata": {},
   "source": [
    "## Image Caption Extraction with BLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da4c3c7-37bf-4f9a-bffe-0e470fe76d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_blip_features(image_path:str, prompt_text:str):\n",
    "    \"\"\"\n",
    "    Extracts image features using BLIP model\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "\n",
    "    Raises:\n",
    "    -------\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image           = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Use BLIP for feature extraction (forward pass only)\n",
    "        inputs          = blip_processor(text           = [prompt_text],\n",
    "                                         images         = image, \n",
    "                                         return_tensors = \"pt\",\n",
    "                                         padding        = True)\n",
    "        \n",
    "        outputs         = blip_model(**inputs)\n",
    "        \n",
    "        text_embedding  = outputs.text_embeds.detach().cpu().numpy()\n",
    "        image_embedding = outputs.image_embeds.detach().cpu().numpy()\n",
    "        return text_embedding, image_embedding\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130399cc-d5d6-418f-b832-1377bdc22959",
   "metadata": {},
   "source": [
    "## Pre-process to make Combined Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d0ad52-0016-4b19-92f5-744758446361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    image_path              = examples[\"image_paths\"][0]\n",
    "    prompt_text             = examples[\"post_heading\"]\n",
    "    \n",
    "    text_embed, image_embed = extract_blip_features(image_path, prompt_text)\n",
    "\n",
    "    combined_input          = (f\"Platform: {examples['platform_name']}. \"\n",
    "                               f\"Post Heading: {examples['post_heading']}. \"\n",
    "                               f\"Post Content: {examples['post_content']}. \"\n",
    "                               f\"BLIP Text Embedding: {text_embed} \"\n",
    "                               f\"BLIP Image Embedding: {image_embed}\"\n",
    "                              )\n",
    "\n",
    "    preprocess_output       = llm_tokenizer(combined_input, \n",
    "                                            truncation = True, \n",
    "                                            padding    = True, \n",
    "                                            max_length = 512)\n",
    "\n",
    "    return preprocess_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d919a68f-8812-48b6-9e7e-dc6a1b93db5e",
   "metadata": {},
   "source": [
    "## Load the pre-trained LLaMA model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b71cf-ca72-4321-b7d3-4063215c4f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_model)\n",
    "llm_model     = AutoModelForCausalLM.from_pretrained(llm_model)\n",
    "\n",
    "# Move LLaMA model to MPS\n",
    "llm_model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a067f3-872c-4a87-a66a-8e332fa7f396",
   "metadata": {},
   "source": [
    "## Fine-tuning configuration for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e0922e-1161-4ff9-8b04-25f73d7ab09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir                  = \"./llama_finetuned\",\n",
    "                                  evaluation_strategy         = \"epoch\",\n",
    "                                  logging_dir                 = \"./logs\",\n",
    "                                  per_device_train_batch_size = 2,\n",
    "                                  num_train_epochs            = 3,\n",
    "                                 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b92e805-8790-4c05-ba40-05e25ff4a48d",
   "metadata": {},
   "source": [
    "## LLM Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d1b3cc-c490-4d06-8427-db61681eff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model         = llm_model,\n",
    "                  args          = training_args,\n",
    "                  train_dataset = tokenized_dataset\n",
    "                 )\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./llama_finetuned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546eb26f-d393-4cbf-9551-b5222599f7f1",
   "metadata": {},
   "source": [
    "## Load Stable Diffusion XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669fab38-a758-4ac3-bf28-355ef99b777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(image_model)\n",
    "pipe.enable_attention_slicing()\n",
    "\n",
    "# Move pipe to MPS\n",
    "pipe.to(device)\n",
    "# Enable LoRA fine-tuning\n",
    "enable_full_lora(pipe.unet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f56af05-2794-405c-a1ba-3642c1f5c2bd",
   "metadata": {},
   "source": [
    "## Perceptual Loss (Feature Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94fbef3-bf0d-43d4-845c-0d938b25f5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        vgg = vgg19(pretrained=True).features[:16].eval()  # Use first few layers\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.vgg = vgg\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, generated_image, target_image):\n",
    "        generated_features = self.vgg(TF.normalize(generated_image, mean=[0.5]*3, std=[0.5]*3))\n",
    "        target_features = self.vgg(TF.normalize(target_image, mean=[0.5]*3, std=[0.5]*3))\n",
    "        return self.criterion(generated_features, target_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32032890-9b88-4df0-9c95-681fcd3ef5a5",
   "metadata": {},
   "source": [
    "## Cross-Modal Alignment Loss (BLIP Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360fbc30-b628-4714-a396-1bad810932a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLIPAlignmentLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.blip_model        = BlipModel.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        self.processor         = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        self.cosine_similarity = nn.CosineSimilarity(dim=1)\n",
    "\n",
    "    def forward(self, generated_image, text_embedding, image_embedding):\n",
    "        # Extract embeddings using BLIP\n",
    "        inputs                    = blip_processor(text           = [\"generated text\"], \n",
    "                                                   images         = generated_image, \n",
    "                                                   return_tensors = \"pt\", \n",
    "                                                   padding        = True)\n",
    "        \n",
    "        outputs                   = blip_model(**inputs)\n",
    "        \n",
    "        generated_text_embedding  = outputs.text_embeds\n",
    "        generated_image_embedding = outputs.image_embeds\n",
    "\n",
    "        # Compute cosine similarities\n",
    "        text_loss                 = 1 - nn.functional.cosine_similarity(text_embedding, generated_text_embedding).mean()\n",
    "        image_loss                = 1 - nn.functional.cosine_similarity(image_embedding, generated_image_embedding).mean()\n",
    "\n",
    "        # Compute average blip alignment loss\n",
    "        blip_alignment_loss       = (text_loss + image_loss) / 2\n",
    "        \n",
    "        return blip_alignment_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15750f5-17eb-411a-98e8-19f24f8fa4e3",
   "metadata": {},
   "source": [
    "## Total Loss Function with BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d77f79-6182-41a0-ae3f-7461b22c3ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TotalBLIPLoss(nn.Module):\n",
    "    def __init__(self, perceptual_loss_weight:float=0.5, blip_loss_weight:float=0.5):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        super(TotalBLIPLoss, self).__init__()\n",
    "        \n",
    "        self.blip_loss_fn           = BLIPAlignmentLoss()\n",
    "        self.perceptual_loss_fn     = PerceptualLoss()\n",
    "        self.perceptual_loss_weight = perceptual_loss_weight\n",
    "        self.blip_loss_weight       = blip_loss_weight\n",
    "        self.resize_transform       = Resize((224, 224))\n",
    "        self.normalize_transform    = Normalize(mean = [0.485, 0.456, 0.406], \n",
    "                                                std  = [0.229, 0.224, 0.225])\n",
    "\n",
    "    def preprocess_image_for_vgg(self, image):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        image = self.resize_transform(image)\n",
    "        return self.normalize_transform(image)\n",
    "\n",
    "    def forward(self, generated_image, target_image, text_embedding, image_embedding):\n",
    "        generated_image = self.preprocess_image_for_vgg(generated_image)\n",
    "        target_image    = self.preprocess_image_for_vgg(target_image)\n",
    "        \n",
    "        blip_loss       = self.blip_loss_fn(generated_image, \n",
    "                                            text_embedding, \n",
    "                                            image_embedding)\n",
    "        perceptual_loss = self.perceptual_loss_fn(generated_image, target_image)\n",
    "\n",
    "        total_loss = self.perceptual_loss_weight * perceptual_loss + self.blip_loss_weight * blip_loss\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103a2cc4-1101-4b99-9c6a-8fbeccf01810",
   "metadata": {},
   "source": [
    "## Prepare optimizer and scheduler for LoRA fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4c6fe8-6f83-4646-a72c-b2edd7dce9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params = pipe.unet.parameters(), \n",
    "                             lr     = 5e-5)\n",
    "\n",
    "scheduler = DDPMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "loss_fn   = TotalBLIPLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fa4218-f3b3-4b3d-81d9-d70fad712323",
   "metadata": {},
   "source": [
    "## Input image features for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d23d29-8436-4f27-a28a-bb92e520daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(training_dataset, \n",
    "                        batch_size = 1, \n",
    "                        shuffle    = True)\n",
    "\n",
    "for step, batch in enumerate(dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    input_image_path        = batch[\"image_paths\"][0]\n",
    "    target_image            = batch[\"target_image\"].to(device)\n",
    "    prompt_text             = batch[\"prompt_text\"]\n",
    "\n",
    "    # Extract embeddings from BLIP\n",
    "    text_embed, image_embed = extract_blip_features(input_image_path, prompt_text)\n",
    "    text_embed              = torch.tensor(text_embed).to(device)\n",
    "    image_embed             = torch.tensor(image_embed).to(device)\n",
    "\n",
    "    # Generate image on MPS\n",
    "    generated_image         = pipe(prompt_text).images[0].to(device)\n",
    "\n",
    "    # Compute loss\n",
    "    loss                    = loss_fn(generated_image = generated_image, \n",
    "                                      target_image    = target_image, \n",
    "                                      text_embedding  = text_embed, \n",
    "                                      image_embedding = image_embed)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "\n",
    "    \n",
    "print(\"Image fine-tuning complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf20062-c3ae-4188-8308-6dc004a9780c",
   "metadata": {},
   "source": [
    "# Final Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cfd7df-3100-4155-8ebb-fcc288b7c639",
   "metadata": {},
   "source": [
    "## Generate Social Media Posts with Integrated Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f640b927-feb4-4fa8-84b9-5c422fbece44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_post_details(text):\n",
    "    hashtags = \" \".join(re.findall(r\"#\\w+\", text))\n",
    "    emojis   = \"ðŸŽ‰ðŸ”¥\"  # You can use an emoji extractor if needed\n",
    "    caption  = text.split(\".\")[0] if text else \"Generated Caption\"\n",
    "    return caption, hashtags, emojis\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fc00d3-5c5b-43aa-acc5-6d5d6ce8fdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_social_media_post(occasion: str, subject: str, platform: str, text_length: int, image_size: str, num_images: int):\n",
    "    \"\"\"\n",
    "    Generate a social media post with text and related images\n",
    "    \"\"\"\n",
    "    # Generate Post Text\n",
    "    prompt         = (f\"Occasion: {occasion}. Subject: {subject}. \"\n",
    "                      f\"Platform: {platform}. Desired length: {text_length} words.\")\n",
    "    \n",
    "    inputs         = llm_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    text_outputs   = llm_model.generate(**inputs, max_new_tokens=150)\n",
    "    generated_text = llm_tokenizer.decode(text_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract caption, hashtags, and emojis from the generated text\n",
    "    caption        = \"Generated Caption Placeholder\"  \n",
    "    hashtags       = \"#GeneratedPlaceholder\"        \n",
    "    emojis         = \"ðŸŽ‰ðŸ”¥\"\n",
    "    \n",
    "    # Generate Related Images\n",
    "    generated_images = list()\n",
    "\n",
    "    for i in range(num_images):\n",
    "        image_prompt    = f\"{subject} for {platform} on {occasion} in {image_size} resolution\"\n",
    "        generated_image = pipe(image_prompt).images[0]\n",
    "        image_path      = f\"./generated_images/post_image_{i+1}.jpg\"\n",
    "        \n",
    "        generated_image.save(image_path)\n",
    "        generated_images.append(image_path)\n",
    "    \n",
    "    output_dict = {\"post_text\" : generated_text,\n",
    "                   \"caption\"   : caption,\n",
    "                   \"hashtags\"  : hashtags,\n",
    "                   \"emojis\"    : emojis,\n",
    "                   \"images\"    : generated_images\n",
    "                  }\n",
    "\n",
    "    return output_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3311f824-0569-4420-826f-7a7978deef91",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ee83b-7189-44df-9f77-5f0059757683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to generate a post\n",
    "result = generate_social_media_post(occasion    = \"Valentine's Day\", \n",
    "                                    subject     = \"Romantic Getaway Packages\", \n",
    "                                    platform    = \"Instagram\", \n",
    "                                    text_length = 100, \n",
    "                                    image_size  = \"1080x1080\", \n",
    "                                    num_images  = 2\n",
    "                                   )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
